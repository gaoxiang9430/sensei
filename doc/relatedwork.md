### Related Works

#### Adversarial attack
L-BFGS [4] is proposed to generate adversarial example by slightly perturbing pixel values, so that the perturbed images is imperceptible by human, but misclassified by neural network. Goodfellow et al. [3] present Fast Gradient Sign (FGS) method to create perturbations by using the sign of loss, so that adversarial examples can be generated quickly. DeepXplore is desined to generate incorrect corner case behaviors, which is slightly different from adversarial examples. DeepXplore first introduces neuron coverage for systematically measuring Deep Learning system, and try to generate misclassified examples by solving joint optimization problem to maximize both differential hehaviors between multiple models and neuron coverage.

A fundamentally different approach from [3,4] was proposed by Sabour et al. [5]. This approach focus not only perturbations designed to produce erroneous class labels, but also the internal layers of DNN representations, to produce adversarial images that differs qualitatively from others. They used L-BFGS to find adversarial images with similar internal representations with the target image and demonstated that adversarial problem is more complex than just mapping the output errors.

#### Models resistant to adversarial attack
To again adversarial attack, the training dataset is often augmented or replaced with adversarial examples produced by various attack strategies, which is known as adversarial training. Madry et al. [2] formalize the adversarial robutstness optimization as a saddle point problem and propose to used gradient decent strategy to solve the saddle point problem.

Furthermore, Madry et al. [1] demonstrate the sample complexity of robust generalization is significantly larger that that of standard generalizaion, so that adversarially robust generalization requires more data.

#### Robustness in spacial space
In contrast to the complicated optimization approaches , Logan et al. [6] demonstrate that adversarial examples are easy to be found by spacial transformation, such as rotations and translations. To improve the robustness of DNN model, they either perform random perturbation of training examples, or replace the training examples with perturbation generated by (1) randomly sample k perturbations (2) choose the one on which the model perform worst. Event though those approaches are able to significantly improve the robustness, it is still easy to find adversarial examples by simple transformations.

DeepTest and DeepRoad are designed to generate realistic image based on Metamorphic testing or Generative adversarial network (GAN). Different from those approaches, we focus on the diversity of the sampless, so that train a more robust model.


[1] Schmidt, Ludwig, et al. "Adversarially Robust Generalization Requires More Data." arXiv preprint arXiv:1804.11285 (2018).
[2] Madry, Aleksander, et al. "Towards deep learning models resistant to adversarial attacks." arXiv preprint arXiv:1706.06083 (2017).
[3] Goodfellow, Ian J. et al. “Explaining and Harnessing Adversarial Examples.” CoRR abs/1412.6572 (2014): n. pag.
[4] Szegedy, Christian, et al. "Intriguing properties of neural networks." arXiv preprint arXiv:1312.6199 (2013).
[5] Sara Sabour, Yanshuai Cao, Fartash Faghri, David J. Fleet: Adversarial Manipulation of Deep Representations. CoRR abs/1511.05122 (2015)
[6] Logan Engstrom, Dimitris Tsipras, Ludwig Schmidt, Aleksander Madry: A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations. CoRR abs/1712.02779 (2017)
[7] DeepTest
[8] DeepRoad
